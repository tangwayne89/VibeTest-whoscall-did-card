# BriefCard PoC æŠ€è¡“å¯¦ä½œæŒ‡å—

## ğŸš€ å¿«é€Ÿé–‹å§‹æŒ‡å—

### 1. Supabase è¨­ç½®

#### 1.1 å»ºç«‹ Supabase å°ˆæ¡ˆ
```bash
# 1. å‰å¾€ https://supabase.com å»ºç«‹æ–°å°ˆæ¡ˆ
# 2. è¨˜éŒ„ä»¥ä¸‹è³‡è¨Šï¼š
#    - Project URL
#    - Anon Key
#    - Service Role Key
```

#### 1.2 è³‡æ–™åº«çµæ§‹è¨­è¨ˆ
```sql
-- ä½¿ç”¨è€…è¡¨ (åˆ©ç”¨ Supabase Auth)
-- auth.users å·²ç”± Supabase æä¾›

-- è³‡æ–™å¤¾è¡¨
CREATE TABLE folders (
  id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
  user_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
  name TEXT NOT NULL,
  color TEXT DEFAULT '#1976D2',
  is_default BOOLEAN DEFAULT FALSE,
  sort_order INTEGER DEFAULT 0,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT timezone('utc'::text, now()) NOT NULL
);

-- é€£çµæ”¶è—è¡¨  
CREATE TABLE bookmarks (
  id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
  user_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
  folder_id UUID REFERENCES folders(id) ON DELETE SET NULL,
  url TEXT NOT NULL,
  title TEXT,
  description TEXT,
  image_url TEXT,
  content_markdown TEXT,
  summary TEXT,
  notes TEXT,
  tags TEXT[],
  created_at TIMESTAMP WITH TIME ZONE DEFAULT timezone('utc'::text, now()) NOT NULL,
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT timezone('utc'::text, now()) NOT NULL
);

-- åˆ†äº«è¡¨
CREATE TABLE shares (
  id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
  bookmark_id UUID REFERENCES bookmarks(id) ON DELETE CASCADE,
  share_token TEXT UNIQUE NOT NULL,
  expires_at TIMESTAMP WITH TIME ZONE,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT timezone('utc'::text, now()) NOT NULL
);

-- æé†’è¡¨
CREATE TABLE reminders (
  id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
  bookmark_id UUID REFERENCES bookmarks(id) ON DELETE CASCADE,
  user_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
  remind_at TIMESTAMP WITH TIME ZONE NOT NULL,
  message TEXT,
  is_sent BOOLEAN DEFAULT FALSE,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT timezone('utc'::text, now()) NOT NULL
);

-- ç´¢å¼•å„ªåŒ–
CREATE INDEX idx_folders_user_id ON folders(user_id);
CREATE INDEX idx_folders_sort_order ON folders(sort_order);
CREATE INDEX idx_bookmarks_user_id ON bookmarks(user_id);
CREATE INDEX idx_bookmarks_folder_id ON bookmarks(folder_id);
CREATE INDEX idx_bookmarks_created_at ON bookmarks(created_at);
CREATE INDEX idx_shares_token ON shares(share_token);
CREATE INDEX idx_reminders_remind_at ON reminders(remind_at) WHERE NOT is_sent;
```

#### 1.3 Row Level Security (RLS) è¨­å®š
```sql
-- å•Ÿç”¨ RLS
ALTER TABLE folders ENABLE ROW LEVEL SECURITY;
ALTER TABLE bookmarks ENABLE ROW LEVEL SECURITY;
ALTER TABLE shares ENABLE ROW LEVEL SECURITY;
ALTER TABLE reminders ENABLE ROW LEVEL SECURITY;

-- è³‡æ–™å¤¾æ”¿ç­–
CREATE POLICY "Users can view own folders" ON folders FOR SELECT USING (auth.uid() = user_id);
CREATE POLICY "Users can insert own folders" ON folders FOR INSERT WITH CHECK (auth.uid() = user_id);
CREATE POLICY "Users can update own folders" ON folders FOR UPDATE USING (auth.uid() = user_id);
CREATE POLICY "Users can delete own folders" ON folders FOR DELETE USING (auth.uid() = user_id);

-- æ›¸ç±¤æ”¿ç­–
CREATE POLICY "Users can view own bookmarks" ON bookmarks FOR SELECT USING (auth.uid() = user_id);
CREATE POLICY "Users can insert own bookmarks" ON bookmarks FOR INSERT WITH CHECK (auth.uid() = user_id);
CREATE POLICY "Users can update own bookmarks" ON bookmarks FOR UPDATE USING (auth.uid() = user_id);
CREATE POLICY "Users can delete own bookmarks" ON bookmarks FOR DELETE USING (auth.uid() = user_id);

-- åˆ†äº«æ”¿ç­– (å…è¨±å…¬é–‹è¨ªå•æœ‰æ•ˆçš„åˆ†äº«)
CREATE POLICY "Anyone can view valid shares" ON shares FOR SELECT USING (expires_at IS NULL OR expires_at > now());

-- æé†’æ”¿ç­–
CREATE POLICY "Users can manage own reminders" ON reminders FOR ALL USING (auth.uid() = user_id);
```

### 2. Crawl4AI æ•´åˆ

#### 2.1 å®‰è£èˆ‡è¨­ç½®
```bash
# å®‰è£ Crawl4AI
pip install crawl4ai

# åˆå§‹åŒ–ç€è¦½å™¨
crawl4ai-setup

# æª¢æŸ¥å®‰è£
crawl4ai-doctor
```

#### 2.2 æ ¸å¿ƒçˆ¬èŸ²æœå‹™å¯¦ä½œ
```python
# services/crawler_service.py
import asyncio
import json
from typing import Optional, Dict, Any
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel

class WebPageData(BaseModel):
    title: str
    description: str
    content_markdown: str
    main_image: Optional[str] = None
    favicon: Optional[str] = None
    author: Optional[str] = None
    publish_date: Optional[str] = None
    reading_time: Optional[int] = None  # åˆ†é˜
    language: Optional[str] = None

class CrawlerService:
    def __init__(self, deepseek_api_key: str):
        self.deepseek_api_key = deepseek_api_key
        
    async def extract_page_data(self, url: str) -> Dict[str, Any]:
        """æå–ç¶²é å®Œæ•´è³‡è¨Š"""
        
        # è¨­å®š LLM æå–ç­–ç•¥
        llm_strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(
                provider="deepseek/deepseek-chat",  # ä½¿ç”¨ DeepSeek é™ä½æˆæœ¬
                api_token=self.deepseek_api_key
            ),
            schema=WebPageData.model_json_schema(),
            extraction_type="schema",
            instruction="""
            è«‹å¾ç¶²é å…§å®¹ä¸­æå–ä»¥ä¸‹è³‡è¨Šï¼š
            - title: æ–‡ç« æ¨™é¡Œ
            - description: æ–‡ç« æ‘˜è¦æˆ–æè¿°ï¼ˆ100-200å­—ï¼‰
            - content_markdown: ä¸»è¦å…§å®¹è½‰ç‚º markdown æ ¼å¼
            - main_image: ä¸»è¦åœ–ç‰‡ URL
            - favicon: ç¶²ç«™åœ–æ¨™ URL
            - author: ä½œè€…åç¨±
            - publish_date: ç™¼å¸ƒæ—¥æœŸ
            - reading_time: é ä¼°é–±è®€æ™‚é–“ï¼ˆåˆ†é˜ï¼‰
            - language: å…§å®¹èªè¨€ï¼ˆzh, en, jaç­‰ï¼‰
            
            è«‹ç¢ºä¿æå–çš„å…§å®¹ä¹¾æ·¨ã€çµæ§‹åŒ–ï¼Œé©åˆå¾ŒçºŒè™•ç†ã€‚
            """,
            chunk_token_threshold=8000,
            overlap_rate=0.1,
            apply_chunking=True,
            extra_args={"temperature": 0.1, "max_tokens": 4000}
        )
        
        # çˆ¬èŸ²é…ç½®
        crawl_config = CrawlerRunConfig(
            extraction_strategy=llm_strategy,
            cache_mode=CacheMode.BYPASS,
            # åçˆ¬èŸ²è¨­å®š
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            },
            wait_for_images=True,
            remove_overlay_elements=True,
            process_iframes=True
        )
        
        browser_cfg = BrowserConfig(
            headless=True,
            viewport_width=1920,
            viewport_height=1080
        )
        
        try:
            async with AsyncWebCrawler(config=browser_cfg) as crawler:
                result = await crawler.arun(url=url, config=crawl_config)
                
                if result.success:
                    extracted_data = json.loads(result.extracted_content)
                    return {
                        "success": True,
                        "data": extracted_data,
                        "raw_content": result.markdown,
                        "images": result.media.get("images", []),
                        "links": result.links
                    }
                else:
                    return {
                        "success": False,
                        "error": result.error_message
                    }
                    
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    async def generate_summary(self, content: str, max_length: int = 200) -> str:
        """ä½¿ç”¨ AI ç”Ÿæˆå…§å®¹æ‘˜è¦"""
        # é€™è£¡å¯ä»¥æ•´åˆ OpenAI æˆ– DeepSeek API
        # æš«æ™‚è¿”å›æˆªæ–·å…§å®¹
        return content[:max_length] + "..." if len(content) > max_length else content
```

### 3. FastAPI å¾Œç«¯æ¶æ§‹

#### 3.1 ä¸»è¦ API çµæ§‹
```python
# main.py
from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from supabase import create_client, Client
import os
from services.crawler_service import CrawlerService
from services.line_service import LineService

app = FastAPI(title="BriefCard API")

# CORS è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # PoC éšæ®µï¼Œç”Ÿç”¢ç’°å¢ƒéœ€è¦é™åˆ¶
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Supabase å®¢æˆ¶ç«¯
supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_ANON_KEY")
)

# æœå‹™å¯¦ä¾‹
crawler_service = CrawlerService(os.getenv("DEEPSEEK_API_KEY"))
line_service = LineService(os.getenv("LINE_CHANNEL_SECRET"), os.getenv("LINE_CHANNEL_ACCESS_TOKEN"))

@app.post("/api/bookmarks")
async def create_bookmark(url: str, user_id: str):
    """å‰µå»ºæ–°æ›¸ç±¤"""
    try:
        # çˆ¬å–ç¶²é è³‡è¨Š
        crawl_result = await crawler_service.extract_page_data(url)
        
        if not crawl_result["success"]:
            raise HTTPException(status_code=400, detail="ç„¡æ³•è™•ç†æ­¤ç¶²å€")
        
        page_data = crawl_result["data"]
        
        # å„²å­˜åˆ° Supabase
        bookmark_data = {
            "user_id": user_id,
            "url": url,
            "title": page_data["title"],
            "description": page_data["description"],
            "content_markdown": page_data["content_markdown"],
            "image_url": page_data["main_image"]
        }
        
        result = supabase.table("bookmarks").insert(bookmark_data).execute()
        
        return {"success": True, "bookmark": result.data[0]}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/line/webhook")
async def line_webhook(request: dict):
    """LINE Bot webhook è™•ç†"""
    return await line_service.handle_webhook(request)
```

### 4. LINE Bot æ•´åˆ

#### 4.1 LINE æœå‹™å¯¦ä½œ
```python
# services/line_service.py
from linebot import LineBotApi, WebhookHandler
from linebot.models import MessageEvent, TextMessage, FlexSendMessage
import re
import json

class LineService:
    def __init__(self, channel_secret: str, channel_access_token: str):
        self.line_bot_api = LineBotApi(channel_access_token)
        self.handler = WebhookHandler(channel_secret)
        
    def is_url(self, text: str) -> bool:
        """æª¢æ¸¬æ˜¯å¦ç‚º URL"""
        url_pattern = r'https?://[^\s]+'
        return bool(re.search(url_pattern, text))
    
    def create_bookmark_card(self, bookmark_data: dict) -> dict:
        """å‰µå»ºæ›¸ç±¤ Flex Message å¡ç‰‡ - Phase 1 è¨­è¨ˆ"""
        # å…§æ–‡æˆªæ–·ç­–ç•¥ï¼šä½¿ç”¨ä¸»è¦å…§æ–‡å‰ 100 å­—
        main_content = bookmark_data.get('content_markdown', bookmark_data.get('description', ''))
        content_preview = main_content[:100] + "..." if len(main_content) > 100 else main_content
        
        # åœ–ç‰‡ fallback ç­–ç•¥ï¼šé¦–åœ– â†’ é è¦½åœ– â†’ icon.png
        image_url = (bookmark_data.get('image_url') or 
                    bookmark_data.get('preview_image') or 
                    'https://your-domain.com/icon.png')
        
        return {
            "type": "flex", 
            "altText": f"ğŸ“‹ {bookmark_data['title']}",
            "contents": {
                "type": "bubble",
                "hero": {
                    "type": "image",
                    "url": image_url,
                    "size": "full",
                    "aspectRatio": "16:9",
                    "aspectMode": "cover"
                },
                "body": {
                    "type": "box",
                    "layout": "vertical",
                    "contents": [
                        {
                            "type": "text",
                            "text": bookmark_data['title'],
                            "weight": "bold",
                            "size": "lg",
                            "wrap": True
                        },
                        {
                            "type": "text",
                            "text": content_preview,
                            "size": "sm",
                            "color": "#666666",
                            "wrap": True,
                            "margin": "md"
                        },
                        {
                            "type": "button",
                            "style": "primary",
                            "action": {
                                "type": "uri",
                                "uri": f"https://your-liff-app.com/edit/{bookmark_data['id']}",
                                "label": "âœï¸ ç·¨è¼¯å¡ç‰‡"
                            },
                            "margin": "md"
                        }
                    ]
                },
                "footer": {
                    "type": "box",
                    "layout": "horizontal",
                    "spacing": "sm",
                    "contents": [
                        {
                            "type": "button",
                            "style": "secondary",
                            "action": {
                                "type": "uri",
                                "uri": bookmark_data['url'],
                                "label": "ğŸ“– é–±è®€åŸæ–‡"
                            }
                        },
                        {
                            "type": "button",
                            "style": "secondary", 
                            "action": {
                                "type": "postback",
                                "data": f"save|{bookmark_data['id']}",
                                "label": "ğŸ’¾ ä¿å­˜æ›¸ç±¤"
                            }
                        }
                    ]
                }
            }
        }
```

### 5. éƒ¨ç½²è¨­å®š

#### 5.1 ç’°å¢ƒè®Šæ•¸è¨­å®š
```bash
# .env
SUPABASE_URL=your_supabase_url
SUPABASE_ANON_KEY=your_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key

LINE_CHANNEL_SECRET=your_line_channel_secret
LINE_CHANNEL_ACCESS_TOKEN=your_line_channel_access_token

DEEPSEEK_API_KEY=your_deepseek_api_key
OPENAI_API_KEY=your_openai_api_key (å‚™ç”¨)

# Vercel éƒ¨ç½²æ™‚ä½¿ç”¨
ENVIRONMENT=production
```

#### 5.2 Vercel éƒ¨ç½²é…ç½®
```json
// vercel.json
{
  "version": 2,
  "builds": [
    {
      "src": "main.py",
      "use": "@vercel/python"
    }
  ],
  "routes": [
    {
      "src": "/(.*)",
      "dest": "main.py"
    }
  ],
  "env": {
    "SUPABASE_URL": "@supabase-url",
    "SUPABASE_ANON_KEY": "@supabase-anon-key"
  }
}
```

### 6. é–‹ç™¼æµç¨‹å»ºè­°

#### 6.1 PoC é–‹ç™¼é †åº
1. **Day 1-2**: è¨­å®š Supabase å°ˆæ¡ˆå’Œè³‡æ–™åº«çµæ§‹
2. **Day 3-4**: å¯¦ä½œåŸºæœ¬çš„ Crawl4AI çˆ¬èŸ²åŠŸèƒ½
3. **Day 5-6**: æ•´åˆ FastAPI å¾Œç«¯ API
4. **Day 7-8**: å¯¦ä½œ LINE Bot åŸºæœ¬åŠŸèƒ½
5. **Day 9-10**: æ¸¬è©¦å’Œå„ªåŒ–

#### 6.2 æ¸¬è©¦ç­–ç•¥
```python
# æ¸¬è©¦ç”¨ä¾‹
test_urls = [
    "https://example.com/article",
    "https://medium.com/@test/article", 
    "https://github.com/repo/readme",
    "https://news.ycombinator.com/item?id=123"
]

# æ•ˆèƒ½æ¸¬è©¦
import time
async def performance_test():
    start_time = time.time()
    result = await crawler_service.extract_page_data(test_url)
    end_time = time.time()
    print(f"è™•ç†æ™‚é–“: {end_time - start_time:.2f} ç§’")
```

## ğŸ’¡ æˆæœ¬å„ªåŒ–å»ºè­°

1. **DeepSeek API**: æ¯” OpenAI ä¾¿å®œç´„ 90%ï¼ŒPoC éšæ®µå»ºè­°ä½¿ç”¨
2. **Supabase å…è²»å±¤**: å¯æ”¯æŒåˆæœŸé–‹ç™¼å’Œæ¸¬è©¦
3. **Vercel å…è²»å±¤**: é©åˆ PoC éƒ¨ç½²
4. **å¿«å–ç­–ç•¥**: å°ç›¸åŒ URL åœ¨ 24 å°æ™‚å…§ä¸é‡è¤‡çˆ¬å–

## ğŸš¨ æ³¨æ„äº‹é …

1. **åçˆ¬èŸ²**: æŸäº›ç¶²ç«™å¯èƒ½éœ€è¦ç‰¹æ®Šè™•ç†
2. **é€Ÿç‡é™åˆ¶**: å»ºè­°åŠ å…¥è«‹æ±‚é–“éš”é¿å…è¢«å°
3. **éŒ¯èª¤è™•ç†**: ç¢ºä¿çˆ¬èŸ²å¤±æ•—æ™‚æœ‰é©ç•¶çš„å›é¥‹
4. **è³‡æ–™æ¸…ç†**: AI æå–çš„è³‡æ–™éœ€è¦é©—è­‰å’Œæ¸…ç†

æº–å‚™å¥½é–‹å§‹å¯¦ä½œäº†å—ï¼Ÿæˆ‘å»ºè­°å¾ Supabase è¨­å®šé–‹å§‹ï¼