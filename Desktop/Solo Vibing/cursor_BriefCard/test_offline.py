#!/usr/bin/env python3
"""
BriefCard - é›¢ç·šæ¸¬è©¦æ¨¡å¼
åœ¨æ²’æœ‰å¤–éƒ¨ API çš„æƒ…æ³ä¸‹æ¸¬è©¦æ ¸å¿ƒåŠŸèƒ½
"""

import asyncio
import json
from datetime import datetime
from typing import Dict, Any

# æœ¬åœ°åŒ¯å…¥ï¼ˆä¸ä¾è³´å¤–éƒ¨ APIï¼‰
from crawler_service import WebCrawlerService

class OfflineTest:
    """é›¢ç·šæ¸¬è©¦é¡"""
    
    def __init__(self):
        self.crawler = WebCrawlerService()
    
    async def test_crawler_only(self, url: str) -> Dict[str, Any]:
        """åƒ…æ¸¬è©¦çˆ¬èŸ²åŠŸèƒ½ï¼Œä¸ä¾è³´å…¶ä»–æœå‹™"""
        print(f"ğŸ•·ï¸ æ¸¬è©¦çˆ¬å–: {url}")
        print("-" * 50)
        
        try:
            start_time = datetime.now()
            result = await self.crawler.extract_content(url)
            end_time = datetime.now()
            
            duration = (end_time - start_time).total_seconds()
            
            if result and result.get("success"):
                print("âœ… çˆ¬å–æˆåŠŸï¼")
                print(f"ğŸ“„ æ¨™é¡Œ: {result.get('title', 'ç„¡æ¨™é¡Œ')}")
                print(f"ğŸ“ æè¿°: {result.get('description', 'ç„¡æè¿°')[:100]}...")
                print(f"ğŸ–¼ï¸ åœ–ç‰‡: {result.get('image_url', 'ç„¡åœ–ç‰‡')}")
                print(f"ğŸ“Š å­—æ•¸: {result.get('word_count', 0)} å­—")
                print(f"â±ï¸ è€—æ™‚: {duration:.2f} ç§’")
                print(f"ğŸ”— æœ€çµ‚ URL: {result.get('url')}")
                
                # é¡¯ç¤ºå…§å®¹é è¦½
                content = result.get('content_markdown', '')
                if content:
                    print(f"\nğŸ“ƒ å…§å®¹é è¦½:")
                    print("-" * 30)
                    print(content[:300] + "..." if len(content) > 300 else content)
                
                return {"status": "success", "result": result, "duration": duration}
            else:
                print("âŒ çˆ¬å–å¤±æ•—")
                print(f"éŒ¯èª¤: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
                return {"status": "failed", "error": result.get('error'), "duration": duration}
                
        except Exception as e:
            print(f"âŒ æ¸¬è©¦ç•°å¸¸: {e}")
            return {"status": "error", "error": str(e)}
    
    async def test_multiple_sites(self):
        """æ¸¬è©¦å¤šå€‹ç¶²ç«™"""
        test_urls = [
            "https://httpbin.org/html",  # ç°¡å–®çš„æ¸¬è©¦é é¢
            "https://example.com",       # åŸºæœ¬ç¯„ä¾‹é é¢  
            "https://github.com",        # è¼ƒè¤‡é›œçš„ç¾ä»£ç¶²ç«™
            "https://news.ycombinator.com",  # ç°¡å–®ä½†æœ‰å…§å®¹çš„ç¶²ç«™
        ]
        
        print("ğŸ§ª å¤šç¶²ç«™çˆ¬å–æ¸¬è©¦")
        print("=" * 60)
        
        results = {}
        
        for i, url in enumerate(test_urls, 1):
            print(f"\n[{i}/{len(test_urls)}] æ¸¬è©¦ç¶²ç«™")
            result = await self.test_crawler_only(url)
            results[url] = result
            
            # åœ¨æ¸¬è©¦é–“ç¨ä½œæš«åœ
            await asyncio.sleep(2)
        
        # çµ±è¨ˆçµæœ
        print("\nğŸ“Š æ¸¬è©¦ç¸½çµ")
        print("=" * 60)
        
        successful = len([r for r in results.values() if r["status"] == "success"])
        total = len(results)
        
        print(f"ç¸½æ¸¬è©¦: {total}")
        print(f"æˆåŠŸ: {successful}")
        print(f"å¤±æ•—: {total - successful}")
        print(f"æˆåŠŸç‡: {(successful/total*100):.1f}%")
        
        # é¡¯ç¤ºå¤±æ•—çš„ç¶²ç«™
        failed_sites = [(url, result) for url, result in results.items() 
                       if result["status"] != "success"]
        
        if failed_sites:
            print(f"\nâŒ å¤±æ•—çš„ç¶²ç«™:")
            for url, result in failed_sites:
                print(f"   {url}: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
        
        return results
    
    def mock_ai_analysis(self, title: str, content: str) -> Dict[str, Any]:
        """æ¨¡æ“¬ AI åˆ†æï¼ˆé›¢ç·šç‰ˆæœ¬ï¼‰"""
        # ç°¡å–®çš„é—œéµè©æå–
        words = content.lower().split()
        common_words = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'must']
        
        # çµ±è¨ˆè©é »ä¸¦éæ¿¾å¸¸è¦‹è©
        word_freq = {}
        for word in words:
            clean_word = ''.join(c for c in word if c.isalnum())
            if len(clean_word) > 3 and clean_word not in common_words:
                word_freq[clean_word] = word_freq.get(clean_word, 0) + 1
        
        # å–å‰5å€‹æœ€å¸¸è¦‹çš„è©ä½œç‚ºé—œéµè©
        keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        keywords = [word for word, _ in keywords]
        
        # ç”Ÿæˆç°¡å–®æ‘˜è¦ï¼ˆå–å‰å…©å¥ï¼‰
        sentences = content.split('.')
        summary = '. '.join(sentences[:2]).strip()
        if summary and not summary.endswith('.'):
            summary += '.'
        
        # ç°¡å–®åˆ†é¡
        tech_keywords = ['python', 'javascript', 'code', 'development', 'api', 'software', 'github']
        news_keywords = ['news', 'report', 'today', 'breaking', 'update']
        
        if any(kw in content.lower() for kw in tech_keywords):
            category = "æŠ€è¡“/ç§‘æŠ€"
        elif any(kw in content.lower() for kw in news_keywords):
            category = "æ–°èæ™‚äº‹"
        else:
            category = "å…¶ä»–"
        
        return {
            "summary": summary[:200] + "..." if len(summary) > 200 else summary,
            "keywords": keywords,
            "category": category
        }
    
    async def test_full_pipeline_offline(self, url: str):
        """æ¸¬è©¦å®Œæ•´çš„é›¢ç·šè™•ç†æµç¨‹"""
        print(f"ğŸ”„ å®Œæ•´é›¢ç·šæ¸¬è©¦: {url}")
        print("=" * 60)
        
        # 1. çˆ¬å–å…§å®¹
        crawl_result = await self.test_crawler_only(url)
        
        if crawl_result["status"] != "success":
            return crawl_result
        
        content_data = crawl_result["result"]
        
        # 2. æ¨¡æ“¬ AI åˆ†æ
        print(f"\nğŸ¤– æ¨¡æ“¬ AI åˆ†æ...")
        ai_analysis = self.mock_ai_analysis(
            content_data.get("title", ""),
            content_data.get("content_markdown", "")
        )
        
        print(f"ğŸ“ æ‘˜è¦: {ai_analysis['summary']}")
        print(f"ğŸ·ï¸ é—œéµè©: {', '.join(ai_analysis['keywords'])}")
        print(f"ğŸ“‚ åˆ†é¡: {ai_analysis['category']}")
        
        # 3. æ¨¡æ“¬æœ€çµ‚æ›¸ç±¤è³‡æ–™
        bookmark_data = {
            "id": "test-bookmark-123",
            "url": content_data.get("url"),
            "title": content_data.get("title"),
            "description": content_data.get("description"),
            "image_url": content_data.get("image_url"),
            "summary": ai_analysis["summary"],
            "keywords": ai_analysis["keywords"],
            "category": ai_analysis["category"],
            "created_at": datetime.now().isoformat(),
            "status": "completed"
        }
        
        print(f"\nğŸ’¾ æ¨¡æ“¬æ›¸ç±¤è³‡æ–™:")
        print(json.dumps(bookmark_data, indent=2, ensure_ascii=False))
        
        return {"status": "success", "bookmark": bookmark_data}

async def main():
    """ä¸»æ¸¬è©¦ç¨‹å¼"""
    tester = OfflineTest()
    
    print("ğŸ§ª BriefCard é›¢ç·šåŠŸèƒ½æ¸¬è©¦")
    print("=" * 60)
    print("é€™å€‹æ¸¬è©¦ä¸éœ€è¦ Supabase æˆ– DeepSeek API")
    print("åƒ…æ¸¬è©¦æ ¸å¿ƒçš„çˆ¬èŸ²å’Œè™•ç†é‚è¼¯")
    print("=" * 60)
    
    # é¸æ“‡æ¸¬è©¦æ¨¡å¼
    print("\né¸æ“‡æ¸¬è©¦æ¨¡å¼:")
    print("1. å–®ä¸€ç¶²ç«™å®Œæ•´æ¸¬è©¦")
    print("2. å¤šç¶²ç«™çˆ¬å–æ¸¬è©¦") 
    print("3. è‡ªè¨‚ç¶²å€æ¸¬è©¦")
    
    choice = input("\nè«‹é¸æ“‡ (1-3): ")
    
    if choice == "1":
        await tester.test_full_pipeline_offline("https://httpbin.org/html")
    elif choice == "2":
        await tester.test_multiple_sites()
    elif choice == "3":
        url = input("è«‹è¼¸å…¥è¦æ¸¬è©¦çš„ç¶²å€: ")
        await tester.test_full_pipeline_offline(url)
    else:
        print("ç„¡æ•ˆé¸æ“‡ï¼ŒåŸ·è¡Œé è¨­æ¸¬è©¦...")
        await tester.test_full_pipeline_offline("https://httpbin.org/html")

if __name__ == "__main__":
    asyncio.run(main())