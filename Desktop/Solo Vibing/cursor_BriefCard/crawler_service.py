#!/usr/bin/env python3
"""
BriefCard - çˆ¬èŸ²æœå‹™
ä½¿ç”¨ Crawl4AI é€²è¡Œç¶²é å…§å®¹æå–
"""

import asyncio
import time
from typing import Optional, Dict, Any
import logging
from urllib.parse import urlparse
import re

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from config import settings

logger = logging.getLogger(__name__)

class WebCrawlerService:
    """ç¶²é çˆ¬èŸ²æœå‹™é¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬èŸ²æœå‹™"""
        self.browser_config = BrowserConfig(
            headless=True,
            viewport_width=1920,
            viewport_height=1080,
            # åçˆ¬èŸ²è¨­å®š
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        
        # ä½¿ç”¨æœ€ç°¡å–®ã€æœ€ç©©å®šçš„è¨­å®š
        self.crawl_config = CrawlerRunConfig(
            # èª¿æ•´è¶…æ™‚è¨­å®š
            page_timeout=15000,  # 15ç§’è¶…æ™‚
            # ä¸ç­‰å¾…ä»»ä½•ç‰¹å®šäº‹ä»¶ï¼Œç«‹å³ç²å–å…§å®¹
            # wait_for åƒæ•¸å®Œå…¨ç§»é™¤ï¼Œä½¿ç”¨é»˜èªè¡Œç‚º
            # é—œé–‰æ‰€æœ‰å¯èƒ½é€ æˆå•é¡Œçš„é¸é …
            remove_overlay_elements=False,
            simulate_user=False,
            override_navigator=False,
            delay_before_return_html=3.0,  # çµ¦å…§å®¹æ›´å¤šæ™‚é–“è¼‰å…¥
        )
    
    def is_valid_url(self, url: str) -> bool:
        """é©—è­‰ URL æ˜¯å¦æœ‰æ•ˆ"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except Exception:
            return False
    
    def clean_url(self, url: str) -> str:
        """æ¸…ç†å’Œæ¨™æº–åŒ– URL"""
        # ç§»é™¤è¿½è¹¤åƒæ•¸
        url = re.sub(r'[?&](utm_[^&]*|fbclid|gclid|_ga|ref)[^&]*', '', url)
        url = re.sub(r'[?&]$', '', url)  # ç§»é™¤æœ«å°¾çš„ ? æˆ– &
        
        # ç¢ºä¿æœ‰å”è­°
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        return url
    
    async def extract_content(self, url: str) -> Optional[Dict[str, Any]]:
        """
        æå–ç¶²é å…§å®¹
        
        Args:
            url: è¦çˆ¬å–çš„ç¶²å€
            
        Returns:
            Dict åŒ…å«æå–çš„å…§å®¹è³‡è¨Šï¼Œæˆ– None å¦‚æœå¤±æ•—
        """
        
        # é©—è­‰å’Œæ¸…ç† URL
        if not self.is_valid_url(url):
            logger.error(f"âŒ ç„¡æ•ˆçš„ URL: {url}")
            return None
        
        cleaned_url = self.clean_url(url)
        logger.info(f"ğŸ•·ï¸ é–‹å§‹çˆ¬å–: {cleaned_url}")
        
        start_time = time.time()
        
        try:
            async with AsyncWebCrawler(config=self.browser_config) as crawler:
                result = await crawler.arun(
                    url=cleaned_url,
                    config=self.crawl_config
                )
                
                if not result.success:
                    error_msg = getattr(result, 'error_message', 'æœªçŸ¥éŒ¯èª¤')
                    logger.error(f"âŒ çˆ¬å–å¤±æ•—: {error_msg}")
                    return {"error": error_msg, "url": cleaned_url, "success": False}
                
                # å®‰å…¨æå–åŸºæœ¬è³‡è¨Šï¼Œè™•ç†æ‰€æœ‰å¯èƒ½çš„ None å€¼
                metadata = result.metadata or {}
                title_raw = metadata.get("title") or ""
                desc_raw = metadata.get("description") or ""
                
                content_data = {
                    "url": cleaned_url,
                    "title": (title_raw.strip() if title_raw else "") or self._extract_title_from_content(result.markdown),
                    "description": (desc_raw.strip() if desc_raw else "") or self._extract_description_from_content(result.markdown),
                    "image_url": self._extract_main_image(metadata),
                    "content_markdown": self._truncate_content(result.markdown or ""),
                    "content_text": self._truncate_content(result.cleaned_html or ""),
                    "author": metadata.get("author") or "",
                    "publish_date": metadata.get("published_time") or "",
                    "site_name": metadata.get("site_name") or self._extract_domain(cleaned_url),
                    "success": True,
                    "crawl_duration": round(time.time() - start_time, 2),
                    "word_count": len((result.markdown or "").split()),
                    "status_code": getattr(result, 'status_code', 200)
                }
                
                logger.info(f"âœ… çˆ¬å–æˆåŠŸ: {cleaned_url} ({content_data['crawl_duration']}s)")
                return content_data
                
        except asyncio.TimeoutError:
            logger.error(f"â° çˆ¬å–é€¾æ™‚: {cleaned_url}")
            return {"error": "çˆ¬å–é€¾æ™‚", "url": cleaned_url, "success": False}
            
        except ConnectionError as e:
            logger.error(f"ğŸŒ ç¶²è·¯é€£ç·šå¤±æ•—: {cleaned_url} - {str(e)}")
            return {"error": f"ç¶²è·¯é€£ç·šå¤±æ•—: {str(e)}", "url": cleaned_url, "success": False}
            
        except Exception as e:
            # è™•ç† Crawl4AI ç‰¹å®šéŒ¯èª¤
            error_str = str(e)
            if "Page.goto: Timeout" in error_str:
                logger.error(f"â° é é¢è¼‰å…¥è¶…æ™‚: {cleaned_url}")
                return {"error": "é é¢è¼‰å…¥è¶…æ™‚", "url": cleaned_url, "success": False}
            elif "net::ERR_" in error_str:
                logger.error(f"ğŸŒ ç¶²è·¯éŒ¯èª¤: {cleaned_url}")
                return {"error": "ç¶²è·¯éŒ¯èª¤", "url": cleaned_url, "success": False}
            else:
                logger.error(f"âŒ çˆ¬å–ç•°å¸¸: {cleaned_url} - {error_str}")
                return {"error": error_str, "url": cleaned_url, "success": False}
    
    def _extract_title_from_content(self, content: str) -> str:
        """å¾å…§å®¹ä¸­æå–æ¨™é¡Œ"""
        if not content:
            return ""
        
        # å°‹æ‰¾ç¬¬ä¸€å€‹ # æ¨™é¡Œ
        lines = content.split('\n')
        for line in lines:
            if line.strip().startswith('# '):
                return line.strip()[2:].strip()
        
        # å¦‚æœæ²’æœ‰ # æ¨™é¡Œï¼Œå–å‰ 100 å­—ç¬¦ä½œç‚ºæ¨™é¡Œ
        return content[:100].strip()
    
    def _extract_description_from_content(self, content: str) -> str:
        """å¾å…§å®¹ä¸­æå–æè¿°"""
        if not content:
            return ""
        
        # ç§»é™¤æ¨™é¡Œï¼Œå–å‰ 300 å­—ç¬¦ä½œç‚ºæè¿°
        lines = content.split('\n')
        text_lines = [line.strip() for line in lines if line.strip() and not line.strip().startswith('#')]
        
        if text_lines:
            description = ' '.join(text_lines)
            return description[:300].strip()
        
        return ""
    
    def _extract_main_image(self, metadata: Dict[str, Any]) -> str:
        """æå–ä¸»è¦åœ–ç‰‡ URL"""
        # å˜—è©¦ä¸åŒçš„åœ–ç‰‡æ¬„ä½
        image_fields = ['og:image', 'twitter:image', 'image', 'thumbnail']
        
        for field in image_fields:
            if field in metadata and metadata[field]:
                return metadata[field]
        
        return ""
    
    def _extract_domain(self, url: str) -> str:
        """å¾ URL æå–ç¶²åŸŸåç¨±"""
        try:
            parsed = urlparse(url)
            return parsed.netloc.replace('www.', '')
        except:
            return ""
    
    def _truncate_content(self, content: str) -> str:
        """æˆªæ–·å…§å®¹åˆ°æŒ‡å®šé•·åº¦"""
        if not content:
            return ""
        
        if len(content) <= settings.max_content_length:
            return content
        
        # åœ¨å–®è©é‚Šç•Œæˆªæ–·
        truncated = content[:settings.max_content_length]
        last_space = truncated.rfind(' ')
        
        if last_space > 0:
            truncated = truncated[:last_space]
        
        return truncated + "..."

# å»ºç«‹å…¨åŸŸçˆ¬èŸ²æœå‹™å¯¦ä¾‹
crawler_service = WebCrawlerService()

# æ¸¬è©¦çˆ¬èŸ²åŠŸèƒ½
async def test_crawler():
    """æ¸¬è©¦çˆ¬èŸ²åŠŸèƒ½"""
    test_url = "https://example.com"
    result = await crawler_service.extract_content(test_url)
    
    if result:
        print("âœ… çˆ¬èŸ²æ¸¬è©¦æˆåŠŸ")
        print(f"æ¨™é¡Œ: {result.get('title')}")
        print(f"æè¿°: {result.get('description')}")
        print(f"è€—æ™‚: {result.get('crawl_duration')}s")
    else:
        print("âŒ çˆ¬èŸ²æ¸¬è©¦å¤±æ•—")

if __name__ == "__main__":
    print("ğŸ•·ï¸ çˆ¬èŸ²æœå‹™æ¸¬è©¦")
    asyncio.run(test_crawler())