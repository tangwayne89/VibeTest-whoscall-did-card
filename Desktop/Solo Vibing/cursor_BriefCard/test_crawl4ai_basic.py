#!/usr/bin/env python3
"""
BriefCard - Crawl4AI åŸºæœ¬åŠŸèƒ½æ¸¬è©¦
é©—è­‰ Crawl4AI å®‰è£å’ŒåŸºæœ¬çˆ¬å–åŠŸèƒ½
"""

import asyncio
import time
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def test_basic_crawling():
    """æ¸¬è©¦åŸºæœ¬ç¶²é çˆ¬å–åŠŸèƒ½"""
    
    print("ğŸ•·ï¸ é–‹å§‹æ¸¬è©¦ Crawl4AI åŸºæœ¬åŠŸèƒ½...")
    
    # æ¸¬è©¦ç¶²å€åˆ—è¡¨
    test_urls = [
        "https://example.com",
        "https://httpbin.org/html",  # ç°¡å–®çš„ HTML æ¸¬è©¦é é¢
        "https://quotes.toscrape.com"  # å°ˆé–€ç”¨æ–¼æ¸¬è©¦çˆ¬èŸ²çš„ç¶²ç«™
    ]
    
    browser_cfg = BrowserConfig(
        headless=True,
        viewport_width=1920,
        viewport_height=1080
    )
    
    try:
        async with AsyncWebCrawler(config=browser_cfg) as crawler:
            
            for i, url in enumerate(test_urls, 1):
                print(f"\nğŸ“„ æ¸¬è©¦ {i}: çˆ¬å– {url}")
                
                start_time = time.time()
                
                try:
                    result = await crawler.arun(url=url)
                    end_time = time.time()
                    
                    if result.success:
                        print(f"âœ… çˆ¬å–æˆåŠŸï¼")
                        print(f"â±ï¸  è€—æ™‚: {end_time - start_time:.2f} ç§’")
                        print(f"ğŸ“ å…§å®¹é•·åº¦: {len(result.markdown)} å­—å…ƒ")
                        print(f"ğŸ–¼ï¸  åœ–ç‰‡æ•¸é‡: {len(result.media.get('images', []))}")
                        print(f"ğŸ”— é€£çµæ•¸é‡: {len(result.links.get('internal', []) + result.links.get('external', []))}")
                        
                        # é¡¯ç¤ºå‰ 200 å­—å…ƒçš„å…§å®¹é è¦½
                        preview = result.markdown[:200].replace('\n', ' ')
                        print(f"ğŸ“ å…§å®¹é è¦½: {preview}...")
                        
                    else:
                        print(f"âŒ çˆ¬å–å¤±æ•—: {result.error_message}")
                        
                except Exception as e:
                    print(f"âŒ çˆ¬å–å‡ºéŒ¯: {e}")
                
                # é¿å…è«‹æ±‚éæ–¼é »ç¹
                await asyncio.sleep(1)
        
        print("\nğŸ‰ Crawl4AI åŸºæœ¬åŠŸèƒ½æ¸¬è©¦å®Œæˆï¼")
        print("ğŸ“ ä¸‹ä¸€æ­¥ï¼šæ¸¬è©¦ AI é©…å‹•çš„å…§å®¹æå–")
        
    except Exception as e:
        print(f"âŒ Crawl4AI æ¸¬è©¦å¤±æ•—: {e}")
        print("ğŸ’¡ è«‹ç¢ºèªå·²æ­£ç¢ºå®‰è£ Crawl4AI ä¸¦åŸ·è¡Œ crawl4ai-setup")

async def test_ai_extraction():
    """æ¸¬è©¦ AI é©…å‹•çš„å…§å®¹æå– (éœ€è¦ API Key)"""
    
    print("\nğŸ¤– æ¸¬è©¦ AI å…§å®¹æå–åŠŸèƒ½...")
    print("ğŸ’¡ é€™å€‹æ¸¬è©¦éœ€è¦ DeepSeek API Key")
    print("   å¦‚æœé‚„æ²’ç”³è«‹ï¼Œå¯ä»¥å…ˆè·³éé€™å€‹æ¸¬è©¦")
    
    # é€™è£¡æˆ‘å€‘åªåšåŸºæœ¬çš„ markdown æå–ï¼Œä¸ä½¿ç”¨ LLM
    # ç­‰ç²å¾— DeepSeek API Key å¾Œå†é€²è¡Œå®Œæ•´æ¸¬è©¦
    
    test_url = "https://quotes.toscrape.com"
    
    try:
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url=test_url)
            
            if result.success:
                print("âœ… åŸºæœ¬å…§å®¹æå–æˆåŠŸ")
                print("ğŸ“‹ å¯ä»¥é€²è¡Œä¸‹ä¸€æ­¥ï¼šæ•´åˆ DeepSeek API")
            else:
                print(f"âŒ å…§å®¹æå–å¤±æ•—: {result.error_message}")
                
    except Exception as e:
        print(f"âŒ AI æå–æ¸¬è©¦å¤±æ•—: {e}")

if __name__ == "__main__":
    print("ğŸ’¡ è«‹å…ˆç¢ºèªå·²å®‰è£ Crawl4AI:")
    print("   pip install crawl4ai")
    print("   crawl4ai-setup")
    print("   crawl4ai-doctor")
    print()
    
    # é‹è¡ŒåŸºæœ¬æ¸¬è©¦
    asyncio.run(test_basic_crawling())
    
    # é‹è¡Œ AI æå–æ¸¬è©¦
    asyncio.run(test_ai_extraction())